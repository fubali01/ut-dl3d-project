{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install pynrrd numpy torch torchvision monai tensorflow wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import monai\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nrrd\n",
    "import torch\n",
    "import PIL\n",
    "import IPython.display\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad(array, target_shape, value=-1000):\n",
    "    shape = array.shape\n",
    "    if len(shape) != len(target_shape):\n",
    "        raise ValueError(\"Target shape does not have same amount of dimensions as input array.\")\n",
    "    for dim in range(len(target_shape)):\n",
    "        if target_shape[dim] < shape[dim]:\n",
    "            raise ValueError(\"Atleast one target dimension is smaller than the current dimension.\")\n",
    "\n",
    "    result = np.ones(target_shape)*value\n",
    "    indices = list()\n",
    "    for dim in range(len(target_shape)):\n",
    "        pad_range = (target_shape[dim]-shape[dim])//2\n",
    "        indices.append([pad_range, pad_range+shape[dim]])\n",
    "    selection = tuple([slice(ind[0], ind[1]) for ind in indices])\n",
    "    result[selection]=array\n",
    "    return result, indices\n",
    "\n",
    "def unpad(array, indices):\n",
    "    if len(indices) != len(array.shape):\n",
    "        raise ValueError(\"Number of dimensions differs between array and indices.\")\n",
    "\n",
    "    selection = tuple([slice(ind[0], ind[1]) for ind in indices])\n",
    "    return array[selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dict_ASOCA(data_path, mode=\"train\"):\n",
    "    # test if mode is correct\n",
    "    if mode not in [\"train\", \"validation\", \"test\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['train', 'validation', 'test']. Current mode is {mode}.\")\n",
    "\n",
    "    # create empty dictionary\n",
    "    dicts = list()\n",
    "\n",
    "    for clazz in [\"Diseased\", \"Normal\"]:\n",
    "        if mode == \"train\":\n",
    "            for index in range(1,17):\n",
    "                image_path = os.path.join(data_path, clazz, \"CTCA\", f\"{clazz}_{index}.nrrd\")\n",
    "                mask_path = os.path.join(data_path, clazz, \"Annotations\", f\"{clazz}_{index}.nrrd\")\n",
    "                dicts.append({\"img\": image_path, \"mask\": mask_path})\n",
    "        if mode == \"validation\":\n",
    "            for index in range(17,21):\n",
    "                image_path = os.path.join(data_path, clazz, \"CTCA\", f\"{clazz}_{index}.nrrd\")\n",
    "                mask_path = os.path.join(data_path, clazz, \"Annotations\", f\"{clazz}_{index}.nrrd\")\n",
    "                dicts.append({\"img\": image_path, \"mask\": mask_path})\n",
    "        if mode == \"test\":\n",
    "            if clazz == \"Diseased\":\n",
    "                for index in range(10,20):\n",
    "                    image_path = os.path.join(data_path, clazz, f\"Testset_Disease\", f\"{index}.nrrd\")\n",
    "                    dicts.append({\"img\": image_path})\n",
    "            else:\n",
    "                for index in range(10):\n",
    "                    image_path = os.path.join(data_path, clazz, f\"Testset_{clazz}\", f\"{index}.nrrd\")\n",
    "                    dicts.append({\"img\": image_path})\n",
    "    return dicts\n",
    "\n",
    "class LoadASOCAData(monai.transforms.Transform):\n",
    "    \n",
    "    def __init__(self, keys=None):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        dicts = dict()\n",
    "        image = nrrd.read(sample[\"img\"])[0]\n",
    "        image, indices = pad(image, (512,512,352))\n",
    "        dicts[\"img\"] = torch.tensor(image).unsqueeze(0).float()\n",
    "        dicts[\"indices\"] = indices\n",
    "        dicts[\"sample\"] = sample # For verifying e.g. padding\n",
    "        if \"mask\" in sample.keys():\n",
    "            mask = nrrd.read(sample[\"mask\"])[0]\n",
    "            mask, indices = pad(mask, (512,512,352), value=0)\n",
    "            dicts[\"mask\"] = torch.tensor(mask).unsqueeze(0).long()\n",
    "        return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adjust cache_rate based on how much memory you have. test_dataset is only used for visualization / at end, so dont need to cache it\n",
    "# Look at prob\n",
    "\n",
    "train_transform = monai.transforms.Compose([\n",
    "    LoadASOCAData(),\n",
    "    monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "    monai.transforms.RandFlipd(keys=['img', 'mask'], prob=0.5, spatial_axis=1),\n",
    "    monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=0.5, mode=['bilinear', 'nearest'])\n",
    "])\n",
    "\n",
    "val_transform = monai.transforms.Compose([\n",
    "    LoadASOCAData(),\n",
    "    monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "])\n",
    "\n",
    "train_dataset = monai.data.CacheDataset(build_dict_ASOCA(\"ASOCA\", mode=\"train\"), transform=train_transform)\n",
    "validation_dataset = monai.data.CacheDataset(build_dict_ASOCA(\"ASOCA\", mode=\"validation\"), transform=val_transform)\n",
    "test_dataset = monai.data.CacheDataset(build_dict_ASOCA(\"ASOCA\", mode=\"test\"), transform=LoadASOCAData(), cache_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !!! Will not work anymore since transforms are applied !!!\n",
    "# verifying padding and unpadding\n",
    "image = nrrd.read(\"ASOCA/Normal/CTCA/Normal_3.nrrd\")[0]\n",
    "print(train_dataset[0][\"sample\"]) # should be same as above\n",
    "image2 = unpad(train_dataset[0][\"img\"].numpy().squeeze().astype(np.int32), train_dataset[0][\"indices\"])\n",
    "(image == image2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "color_actual = [0, 0.5, 0]\n",
    "color_predicted = [0, 0, 0.5]\n",
    "\n",
    "def RGB_mask(mask, color):\n",
    "    result = np.zeros((*mask.shape,3))\n",
    "    for i in range(3):\n",
    "        result[...,i]=color[i]*mask\n",
    "    return result\n",
    "\n",
    "\n",
    "def RGB_image(image):\n",
    "    result = image-image.min() # [a,b] -> [0, b-a]\n",
    "    result = result/result.max() # [0, b-a] -> [0,1] -> [0,255]\n",
    "    return np.repeat(np.reshape(result[:,:,:], [image.shape[0],image.shape[1],image.shape[2],1]),3,axis=3)\n",
    "\n",
    "def visualize_3d_gif(indices, image=None, actual_mask=None, predicted_mask=None, name_gif=\"array.gif\"):\n",
    "    if image is not None:\n",
    "        image = image.numpy().squeeze()\n",
    "        image = unpad(image, indices)\n",
    "        rgb_image = RGB_image(image)\n",
    "        result = rgb_image\n",
    "    for i in range(2):\n",
    "        mask = [actual_mask, predicted_mask][i]\n",
    "        if mask is not None:\n",
    "            mask = mask.numpy().squeeze()\n",
    "            mask = unpad(mask, indices)\n",
    "            rgb_mask = RGB_mask(mask, [color_actual, color_predicted][i])\n",
    "            if result is not None:\n",
    "                result += rgb_mask\n",
    "            else:\n",
    "                result = rgb_mask\n",
    "\n",
    "    result = result/np.max(result)*255\n",
    "    result = result.astype(np.uint8)\n",
    "    images = [PIL.Image.fromarray(result[:,:,index,:]) for index in range(image.shape[2])]\n",
    "    images[0].save(name_gif, save_all=True, append_images=images[1:],loop=0)\n",
    "\n",
    "def visualize_3d_masks(indices, actual_mask=None, predicted_mask=None):\n",
    "    fig=plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    for i in range(2):\n",
    "        mask = [actual_mask, predicted_mask][i]\n",
    "        if mask is not None:\n",
    "            mask = mask.numpy().squeeze()\n",
    "            mask = unpad(mask, indices)\n",
    "            pos = np.where(mask==1)\n",
    "            ax.scatter(pos[0],pos[1],pos[2], color=[color_actual, color_predicted][i])\n",
    "\n",
    "    # multiple figures at different angles?\n",
    "    #ax.view_init(45, 0)\n",
    "    plt.show()    \n",
    "\n",
    "def visualize_histogram(indices, image, mask):\n",
    "    image = image.numpy().squeeze()\n",
    "    image = unpad(image, indices)\n",
    "    mask = mask.numpy().squeeze()\n",
    "    mask = unpad(mask, indices)\n",
    "    fig, axs = plt.subplots(1, 2, tight_layout=True) \n",
    "    axs[0].hist(image[mask==0], bins=20)\n",
    "    axs[0].set_title(\"Background class\")\n",
    "    axs[1].hist(image[mask==1], bins=20)\n",
    "    axs[1].set_title(\"Vessel class\")   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_3d_gif(train_dataset[1][\"indices\"],image=train_dataset[1][\"img\"], actual_mask=train_dataset[1][\"mask\"])\n",
    "display(IPython.display.Image(data=open(\"array.gif\",'rb').read(), format='png'))\n",
    "visualize_3d_masks(train_dataset[1][\"indices\"], actual_mask=train_dataset[1][\"mask\"])\n",
    "visualize_histogram(train_dataset[1][\"indices\"], train_dataset[1][\"img\"],train_dataset[1][\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "train_loader = monai.data.DataLoader(train_dataset, batch_size=1, num_workers=1)\n",
    "validation_loader = monai.data.DataLoader(validation_dataset, batch_size=1, num_workers=1)\n",
    "device = torch.device(\"cuda\")\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(8, 16, 32, 64, 128),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='ASOCAproject',\n",
    "    name='test_1',\n",
    "    config={\n",
    "        'loss function': str(loss_function), \n",
    "        'lr': optimizer.param_groups[0][\"lr\"],\n",
    "        'batch_size': train_loader.batch_size,\n",
    "    }\n",
    ")\n",
    "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n",
    "# For example you should add information on your model...\n",
    "\n",
    "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n",
    "\n",
    "def wandb_masks(mask_output, mask_gt):\n",
    "    \"\"\" Function that generates a mask dictionary in format that W&B requires \"\"\"\n",
    "\n",
    "    # Apply sigmoid to model ouput and round to nearest integer (0 or 1)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    mask_output = sigmoid(mask_output)\n",
    "    mask_output = torch.round(mask_output)\n",
    "\n",
    "    # Transform masks to numpy arrays on CPU\n",
    "    # Note: .squeeze() removes all dimensions with a size of 1 (here, it makes the tensors 2-dimensional)\n",
    "    # Note: .detach() removes a tensor from the computational graph to prevent gradient computation for it\n",
    "    mask_output = mask_output.squeeze().detach().cpu().numpy()\n",
    "    mask_gt = mask_gt.squeeze().detach().cpu().numpy()\n",
    "\n",
    "    # Create mask dictionary with class label and insert masks\n",
    "    class_labels = {1: 'arteries'}\n",
    "    masks = {\n",
    "        'predictions': {'mask_data': mask_output, 'class_labels': class_labels},\n",
    "        'ground truth': {'mask_data': mask_gt, 'class_labels': class_labels}\n",
    "    }\n",
    "    return masks\n",
    "\n",
    "def log_to_wandb(epoch, train_loss, val_loss, batch_data):\n",
    "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n",
    "    # This part is not yet working\n",
    "    # def log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs):\n",
    "    # Create list of images that have segmentation masks for model output and ground truth\n",
    "    #log_imgs = [wandb.Image(img, masks=wandb_masks(mask_output, mask_gt)) for img, mask_output,\n",
    "    #            mask_gt in zip(batch_data['img'], outputs, batch_data['mask'])]\n",
    "\n",
    "    # Send epoch, losses and images to W&B\n",
    "    #wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'results': log_imgs})\n",
    "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "\n",
    "for epoch in tqdm(range(200)):\n",
    "    # training\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    step=0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Step\", leave=False, ncols=100):  # Nested tqdm for training steps\n",
    "        print(step)\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"mask\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() \n",
    "    train_loss = epoch_loss / step\n",
    "    \n",
    "    # validation\n",
    "    step = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_loader, desc=\"Validation Step\", leave=False, ncols=100):  # Nested tqdm for training steps\n",
    "            print(step)\n",
    "            step += 1\n",
    "            model.eval()\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"mask\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss+= loss.item()\n",
    "        val_loss = val_loss / step\n",
    "    \n",
    "    log_to_wandb(epoch, train_loss, val_loss, batch)\n",
    "    print(f\"Epoch {epoch+1}, Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(),os.path.join(f\"Trained\", f\"trainedUNet_epoch{epoch}.pt\"))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(r'Trained/trainedUNet_epoch87.pt'))\n",
    "model.load_state_dict(torch.load(r'TrainedASOCA.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visual_evaluation(sample, model):\n",
    "    \"\"\"\n",
    "    Allow the visual inspection of one sample by plotting the X-ray image, the ground truth (green)\n",
    "    and the segmentation map produced by the network (red).\n",
    "    \n",
    "    Args:\n",
    "        sample (Dict[str, torch.Tensor]): sample composed of an X-ray ('img') and a mask ('mask').\n",
    "        model (torch.nn.Module): trained model to evaluate.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    discrete_transform = monai.transforms.AsDiscrete(logit_thresh=0.5, threshold_values=True)\n",
    "    Softmax = torch.nn.Softmax()\n",
    "    with torch.no_grad():\n",
    "        output = discrete_transform(Softmax(model(sample['img'].to('cuda:1'))).cpu()).squeeze()\n",
    "        output = np.squeeze(output[1, :, :, :])\n",
    "        print(output.shape)\n",
    "    \n",
    "    fig, ax =plt.subplots(1,2, subplot_kw={\"projection\":\"3d\"})\n",
    "\n",
    "    indices = sample['indices']\n",
    "    actual_mask = np.squeeze(sample['mask'])\n",
    "    actual_mask = unpad(actual_mask, indices)\n",
    "    pos = np.where(actual_mask==1)\n",
    "    print(len(pos[0]))\n",
    "    ax[0].scatter(pos[0],pos[1],pos[2], color=color_actual)\n",
    "    \n",
    "    output = unpad(output, indices)\n",
    "    pos2 = np.where(output == 1)\n",
    "    print(len(pos2[0]))\n",
    "    ax[1].scatter(pos2[0],pos2[1],pos2[2], color=color_predicted)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in validation_loader:\n",
    "    visual_evaluation(sample, model.to('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
