{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pynrrd in /home/jovyan/.local/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: monai in /home/jovyan/.local/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
      "Requirement already satisfied: wandb in /home/jovyan/.local/lib/python3.10/site-packages (0.19.8)\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from pynrrd) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.10/dist-packages (from torch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jovyan/.local/lib/python3.10/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/lib/python3/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/jovyan/.local/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/jovyan/.local/lib/python3.10/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: setproctitle in /home/jovyan/.local/lib/python3.10/site-packages (from wandb) (1.3.5)\n",
      "Collecting importlib-resources>=5.12 (from nibabel)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jovyan/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.38.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "Installing collected packages: wrapt, importlib-resources, nibabel\n",
      "\u001b[33m  WARNING: The scripts nib-conform, nib-convert, nib-dicomfs, nib-diff, nib-ls, nib-nifti-dx, nib-roi, nib-stats, nib-tck2trk, nib-trk2tck and parrec2nii are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "otter-grader 6.1.1 requires wrapt<2.0.0,>=1.16.0, but you have wrapt 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed importlib-resources-6.5.2 nibabel-5.3.2 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pynrrd numpy torch torchvision monai tensorflow wandb nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 15:51:34.484841: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-31 15:51:34.522603: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-31 15:51:34.522623: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-31 15:51:34.522641: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-31 15:51:34.530173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-31 15:51:35.439233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import nrrd\n",
    "import torch\n",
    "import PIL\n",
    "import IPython.display\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.transforms import (\n",
    "    LoadImage,\n",
    "    LoadImaged)\n",
    "from monai.inferers import sliding_window_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dict_ASOCA(data_path, mode=\"train\"):\n",
    "    # test if mode is correct\n",
    "    if mode not in [\"train\", \"validation\", \"test\"]:\n",
    "        raise ValueError(f\"Please choose a mode in ['train', 'validation', 'test']. Current mode is {mode}.\")\n",
    "\n",
    "    # create empty dictionary\n",
    "    dicts = list()\n",
    "\n",
    "    for clazz in [\"Diseased\", \"Normal\"]:\n",
    "        if mode == \"train\":\n",
    "            for index in range(1,17):\n",
    "                image_path = os.path.join(data_path, clazz, \"CTCA\", f\"{clazz}_{index}.nrrd\")\n",
    "                mask_path = os.path.join(data_path, clazz, \"Annotations\", f\"{clazz}_{index}.nrrd\")\n",
    "                dicts.append({\"img\": image_path, \"mask\": mask_path})\n",
    "        if mode == \"validation\":\n",
    "            for index in range(17,21):\n",
    "                image_path = os.path.join(data_path, clazz, \"CTCA\", f\"{clazz}_{index}.nrrd\")\n",
    "                mask_path = os.path.join(data_path, clazz, \"Annotations\", f\"{clazz}_{index}.nrrd\")\n",
    "                dicts.append({\"img\": image_path, \"mask\": mask_path})\n",
    "        if mode == \"test\":\n",
    "            if clazz == \"Diseased\":\n",
    "                for index in range(10,20):\n",
    "                    image_path = os.path.join(data_path, clazz, f\"Testset_Disease\", f\"{index}.nrrd\")\n",
    "                    dicts.append({\"img\": image_path})\n",
    "            else:\n",
    "                for index in range(10):\n",
    "                    image_path = os.path.join(data_path, clazz, f\"Testset_{clazz}\", f\"{index}.nrrd\")\n",
    "                    dicts.append({\"img\": image_path})\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dict_ASOCA_secret(data_path):\n",
    "    # create empty dictionary\n",
    "    dicts = list()\n",
    "\n",
    "    for index in range(1,21):\n",
    "        image_path = os.path.join(data_path, f\"{index}.img.nii\")\n",
    "        dicts.append({\"img\": image_path})\n",
    "    return dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([512, 512, 224])\n",
      "image voxel dim: tensor([0.3867, 0.3867, 0.6250], dtype=torch.float64)\n",
      "image shape: torch.Size([512, 512, 168])\n",
      "image voxel dim: tensor([0.4316, 0.4316, 0.6250], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Some examples of image sizes and voxel dimensions\n",
    "\n",
    "loader = LoadImage(dtype=np.float32, image_only=True)\n",
    "image = loader(build_dict_ASOCA(\"ASOCA\", mode=\"train\")[0][\"img\"])\n",
    "# print(f\"input: {train_data_dicts[0]['image']}\")\n",
    "print(f\"image shape: {image.shape}\")\n",
    "print(f\"image voxel dim: {image.pixdim}\")\n",
    "image = loader(build_dict_ASOCA(\"ASOCA\", mode=\"validation\")[2][\"img\"])\n",
    "# print(f\"input: {train_data_dicts[0]['image']}\")\n",
    "print(f\"image shape: {image.shape}\")\n",
    "print(f\"image voxel dim: {image.pixdim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([512, 512, 275])\n",
      "image voxel dim: tensor([0.3613, 0.3613, 0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "loader = LoadImage(dtype=np.float32, image_only=True, reader=\"NibabelReader\")\n",
    "image = loader(build_dict_ASOCA_secret(\"forDLMIA\")[2][\"img\"])\n",
    "print(f\"image shape: {image.shape}\")\n",
    "print(f\"image voxel dim: {image.pixdim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 32/32 [01:26<00:00,  2.70s/it]\n",
      "Loading dataset: 100%|██████████| 8/8 [00:23<00:00,  2.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# adjust cache_rate based on how much memory you have. test_dataset is only used for visualization / at end, so dont need to cache it\n",
    "# Look at the values for pixdim\n",
    "\n",
    "# Training transforms\n",
    "train_transform = monai.transforms.Compose([\n",
    "    # Load the image with monai's image loader:\n",
    "    monai.transforms.LoadImaged(keys=(\"img\", \"mask\"), image_only=False),\n",
    "    # Add channel since transforms expect a channel dimension:\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['img', 'mask'], channel_dim=\"no_channel\"),\n",
    "    # Create uniform voxel spacing:\n",
    "    monai.transforms.Spacingd(keys=[\"img\", \"mask\"], pixdim=(0.4, 0.4, 0.5), mode=(\"bilinear\", \"nearest\")),\n",
    "    # Scale the intensities:\n",
    "    monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "    # Random flip and rotate:\n",
    "    monai.transforms.RandFlipd(keys=['img', 'mask'], prob=0.5, spatial_axis=1),\n",
    "    monai.transforms.RandRotated(keys=['img', 'mask'], range_x=np.pi/4, prob=0.5, mode=['bilinear', 'nearest']),\n",
    "    # Crop to 128×128×128\n",
    "    monai.transforms.RandSpatialCropd(keys=['img', 'mask'], roi_size=[256,256,128], random_size=False)\n",
    "])\n",
    "\n",
    "# Validation transforms\n",
    "val_transform = monai.transforms.Compose([\n",
    "    # Load the image with monai's image loader:\n",
    "    monai.transforms.LoadImaged(keys=(\"img\", \"mask\"), image_only=False),\n",
    "    # Add channel since transforms expect a channel dimension:\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['img', 'mask'], channel_dim=\"no_channel\"),\n",
    "    # Create uniform voxel spacing:\n",
    "    monai.transforms.Spacingd(keys=[\"img\", \"mask\"], pixdim=(0.4, 0.4, 0.5), mode=(\"bilinear\", \"nearest\")),\n",
    "    # Scale the intensities:\n",
    "    monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "    #monai.transforms.DivisiblePadd(keys=['img', 'mask'], k=64)\n",
    "])\n",
    "\n",
    "# Test transforms\n",
    "test_transform = monai.transforms.Compose([\n",
    "    # Load the image with monai's image loader:\n",
    "    monai.transforms.LoadImaged(keys=(\"img\", \"mask\"), image_only=False),\n",
    "    # Add channel since transforms expect a channel dimension:\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['img', 'mask'], channel_dim=\"no_channel\"),\n",
    "    # Create uniform voxel spacing:\n",
    "    monai.transforms.Spacingd(keys=[\"img\", \"mask\"], pixdim=(0.4, 0.4, 0.5), mode=(\"bilinear\", \"nearest\")),\n",
    "    # Scale the intensities:\n",
    "    monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = monai.data.CacheDataset(build_dict_ASOCA(\"ASOCA\", mode=\"train\"), transform=train_transform, num_workers = 8)\n",
    "validation_dataset = monai.data.CacheDataset(build_dict_ASOCA(\"ASOCA\", mode=\"validation\"), transform=val_transform, num_workers =8)\n",
    "#test_dataset = monai.data.CacheDataset(build_dict_ASOCA(\"ASOCA\", mode=\"test\"), transform=test_transform, cache_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Secret test transforms\n",
    "secret_test_transform = monai.transforms.Compose([\n",
    "    # Load the image with monai's image loader:\n",
    "    monai.transforms.LoadImaged(keys=(\"img\"), image_only=True),\n",
    "    # Add channel since transforms expect a channel dimension:\n",
    "    monai.transforms.EnsureChannelFirstd(keys=['img'], channel_dim=\"no_channel\"),\n",
    "    # Create uniform voxel spacing:\n",
    "    monai.transforms.Spacingd(keys=[\"img\"], pixdim=(0.4, 0.4, 0.5), mode=(\"bilinear\")),\n",
    "    # Scale the intensities:\n",
    "    monai.transforms.ScaleIntensityd(keys=['img'],minv=0, maxv=1),\n",
    "])\n",
    "\n",
    "secret_test_dataset = monai.data.CacheDataset(build_dict_ASOCA_secret(\"forDLMIA\"), transform=secret_test_transform, cache_rate=0, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the image torch.Size([1, 256, 256, 128])\n",
      "Voxel dim of the image tensor([0.4000, 0.4000, 0.5000], dtype=torch.float64)\n",
      "Size of the mask torch.Size([1, 256, 256, 128])\n",
      "Voxel dim of the mask tensor([0.4000, 0.4000, 0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Check if the image size and voxel dimension have correctly changed\n",
    "sample_dict = train_dataset[20]\n",
    "print(\"Size of the image\", sample_dict[\"img\"].shape)\n",
    "print(\"Voxel dim of the image\", sample_dict[\"img\"].pixdim)\n",
    "print(\"Size of the mask\", sample_dict[\"mask\"].shape)\n",
    "print(\"Voxel dim of the mask\", sample_dict[\"mask\"].pixdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the image torch.Size([1, 453, 453, 275])\n",
      "Voxel dim of the image tensor([0.4000, 0.4000, 0.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "sample_dict = secret_test_dataset[5]\n",
    "print(\"Size of the image\", sample_dict[\"img\"].shape)\n",
    "print(\"Voxel dim of the image\", sample_dict[\"img\"].pixdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization options\n",
    "color_actual = [0, 0.5, 0]\n",
    "color_predicted = [0, 0, 0.5]\n",
    "\n",
    "def RGB_mask(mask, color):\n",
    "    result = np.zeros((*mask.shape,3))\n",
    "    for i in range(3):\n",
    "        result[...,i]=color[i]*mask\n",
    "    return result\n",
    "\n",
    "def RGB_image(image):\n",
    "    result = image-image.min() # [a,b] -> [0, b-a]\n",
    "    result = result/result.max() # [0, b-a] -> [0,1] -> [0,255]\n",
    "    return np.repeat(np.reshape(result[:,:,:], [image.shape[0],image.shape[1],image.shape[2],1]),3,axis=3)\n",
    "\n",
    "def visualize_3d_gif(image=None, actual_mask=None, predicted_mask=None, name_gif=\"array.gif\"):\n",
    "    if image is not None:\n",
    "        image = image.numpy().squeeze()\n",
    "        rgb_image = RGB_image(image)\n",
    "        result = rgb_image\n",
    "    for i in range(2):\n",
    "        mask = [actual_mask, predicted_mask][i]\n",
    "        if mask is not None:\n",
    "            mask = mask.numpy().squeeze()\n",
    "            rgb_mask = RGB_mask(mask, [color_actual, color_predicted][i])\n",
    "            if result is not None:\n",
    "                result += rgb_mask\n",
    "            else:\n",
    "                result = rgb_mask\n",
    "\n",
    "    result = result/np.max(result)*255\n",
    "    result = result.astype(np.uint8)\n",
    "    images = [PIL.Image.fromarray(result[:,:,index,:]) for index in range(image.shape[2])]\n",
    "    images[0].save(name_gif, save_all=True, append_images=images[1:],loop=0)\n",
    "\n",
    "def visualize_3d_masks(actual_mask=None, predicted_mask=None):\n",
    "    fig=plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    for i in range(2):\n",
    "        mask = [actual_mask, predicted_mask][i]\n",
    "        if mask is not None:\n",
    "            mask = mask.numpy().squeeze()\n",
    "            pos = np.where(mask==1)\n",
    "            ax.scatter(pos[0],pos[1],pos[2], color=[color_actual, color_predicted][i])\n",
    "\n",
    "    # multiple figures at different angles?\n",
    "    #ax.view_init(45, 0)\n",
    "    plt.show()    \n",
    "\n",
    "def visualize_histogram(image, mask):\n",
    "    image = image.numpy().squeeze()\n",
    "    mask = mask.numpy().squeeze()\n",
    "    fig, axs = plt.subplots(1, 2, tight_layout=True) \n",
    "    axs[0].hist(image[mask==0], bins=20)\n",
    "    axs[0].set_title(\"Background class\")\n",
    "    axs[1].hist(image[mask==1], bins=20)\n",
    "    axs[1].set_title(\"Vessel class\")   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize_3d_gif(image=validation_dataset[0][\"img\"], actual_mask=validation_dataset[0][\"mask\"], name_gif=\"val_array.gif\")\n",
    "# display(IPython.display.Image(data=open(\"val_array.gif\",'rb').read(), format='png'))\n",
    "# visualize_3d_masks(actual_mask=validation_dataset[0][\"mask\"])\n",
    "visualize_histogram(validation_dataset[0][\"img\"],validation_dataset[0][\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_histogram_secret(original, secret):\n",
    "    original = original.numpy().squeeze().flatten()\n",
    "    secret = secret.numpy().squeeze().flatten()\n",
    "    fig, axs = plt.subplots(1, 2, tight_layout=True) \n",
    "    axs[0].hist(original, bins=20)\n",
    "    axs[0].set_title(\"Sample from original dataset\")\n",
    "    axs[1].hist(secret, bins=20)\n",
    "    axs[1].set_title(\"Sample from secret test dataset\")   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize_3d_gif(image=secret_test_dataset[0][\"img\"])\n",
    "# display(IPython.display.Image(data=open(\"array.gif\",'rb').read(), format='png'))\n",
    "visualize_histogram_secret(validation_dataset[0][\"img\"], secret_test_dataset[0][\"img\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "train_loader = monai.data.DataLoader(train_dataset, batch_size=4, num_workers=8,pin_memory=torch.cuda.is_available())\n",
    "validation_loader = monai.data.DataLoader(validation_dataset, batch_size=1, num_workers=8,pin_memory=torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:1\")\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(8, 16, 32, 64, 128),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = monai.losses.DiceLoss(to_onehot_y=True, softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project='ASOCAproject',\n",
    "    name='Test_crop256_v3',\n",
    "    config={\n",
    "        'loss function': str(loss_function), \n",
    "        'lr': optimizer.param_groups[0][\"lr\"],\n",
    "        'batch_size': train_loader.batch_size,\n",
    "    }\n",
    ")\n",
    "# Do not hesitate to enrich this list of settings to be able to correctly keep track of your experiments!\n",
    "# For example you should add information on your model...\n",
    "\n",
    "run_id = run.id # We remember here the run ID to be able to write the evaluation metrics\n",
    "\n",
    "def log_to_wandb(epoch, train_loss, val_loss, batch_data):\n",
    "    \"\"\" Function that logs ongoing training variables to W&B \"\"\"\n",
    "    # This part is not yet working\n",
    "    # def log_to_wandb(epoch, train_loss, val_loss, batch_data, outputs):\n",
    "    # Create list of images that have segmentation masks for model output and ground truth\n",
    "    #log_imgs = [wandb.Image(img, masks=wandb_masks(mask_output, mask_gt)) for img, mask_output,\n",
    "    #            mask_gt in zip(batch_data['img'], outputs, batch_data['mask'])]\n",
    "\n",
    "    # Send epoch, losses and images to W&B\n",
    "    #wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'results': log_imgs})\n",
    "    wandb.log({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss})\n",
    "\n",
    "for epoch in tqdm(range(1000)):\n",
    "    # training\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    step=0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Step\", leave=False, ncols=100):  # Nested tqdm for training steps\n",
    "        print(step)\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch[\"img\"].to(device)\n",
    "        labels = batch[\"mask\"].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() \n",
    "    train_loss = epoch_loss / step\n",
    "    \n",
    "    # validation\n",
    "    step = 0\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_loader, desc=\"Validation Step\", leave=False, ncols=100):  # Nested tqdm for training steps\n",
    "            print(step)\n",
    "            step += 1\n",
    "            model.eval()\n",
    "            inputs = batch[\"img\"].to(device)\n",
    "            labels = batch[\"mask\"].to(device)\n",
    "            outputs = sliding_window_inference(inputs, (256, 256, 128), 4, model)\n",
    "            #outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss+= loss.item()\n",
    "        val_loss = val_loss / step\n",
    "    \n",
    "    log_to_wandb(epoch, train_loss, val_loss, batch)\n",
    "    print(f\"Epoch {epoch+1}, Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(),os.path.join(f\"Trained_crop256_v3\", f\"trainedUNet_epoch{epoch}.pt\"))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on CUDA device 7 but torch.cuda.device_count() is 2. Please use torch.load with map_location to map your storages to an existing device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Option to load a previous trained model, check path!\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainedUNet_epoch543.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:265\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 265\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:256\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    254\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on CUDA device \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but torch.cuda.device_count() is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    258\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load with map_location to map your storages \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto an existing device.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on CUDA device 7 but torch.cuda.device_count() is 2. Please use torch.load with map_location to map your storages to an existing device."
     ]
    }
   ],
   "source": [
    "# Option to load a previous trained model, check path!\n",
    "model.load_state_dict(torch.load(r'trainedUNet_epoch543.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visual_evaluation(sample, model):\n",
    "    \"\"\"\n",
    "    Allow the visual inspection of one sample by plotting the X-ray image, the ground truth (green)\n",
    "    and the segmentation map produced by the network (red).\n",
    "    \n",
    "    Args:\n",
    "        sample (Dict[str, torch.Tensor]): sample composed of an X-ray ('img') and a mask ('mask').\n",
    "        model (torch.nn.Module): trained model to evaluate.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inferer = monai.inferers.SlidingWindowInferer(roi_size=[256, 256, 128])\n",
    "    discrete_transform = monai.transforms.AsDiscrete(logit_thresh=0.5, threshold_values=True)\n",
    "    Softmax = torch.nn.Softmax()\n",
    "    with torch.no_grad():\n",
    "        print(sample['img'].shape)\n",
    "        #output = discrete_transform(Softmax(model(sample['img'].to('cuda:1'))).cpu()).squeeze()\n",
    "        output = discrete_transform(Softmax(inferer(sample['img'].to('cuda:6'), network=model).cpu())).squeeze()\n",
    "        output = np.squeeze(output[1, :, :, :])\n",
    "        print(output.shape)\n",
    "    \n",
    "    fig, ax =plt.subplots(1,2, subplot_kw={\"projection\":\"3d\"})\n",
    "\n",
    "    actual_mask = np.squeeze(sample['mask'])\n",
    "    pos = np.where(actual_mask==1)\n",
    "    print(len(pos[0]))\n",
    "    ax[0].scatter(pos[0],pos[1],pos[2], color=color_actual)\n",
    "    \n",
    "    pos2 = np.where(output == 1)\n",
    "    print(len(pos2[0]))\n",
    "    ax[1].scatter(pos2[0],pos2[1],pos2[2], color=color_predicted)\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in validation_loader:\n",
    "    visual_evaluation(sample, model.to('cuda:6'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 31 15:15:23 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A16                     Off |   00000000:1B:00.0 Off |                    0 |\n",
      "|  0%   41C    P0             35W /   62W |    1677MiB /  15356MiB |     81%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A16                     Off |   00000000:1C:00.0 Off |                    0 |\n",
      "|  0%   42C    P0             34W /   62W |    3260MiB /  15356MiB |     49%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A16                     Off |   00000000:1D:00.0 Off |                    0 |\n",
      "|  0%   37C    P0             34W /   62W |    2738MiB /  15356MiB |     53%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A16                     Off |   00000000:1E:00.0 Off |                    0 |\n",
      "|  0%   35C    P0             32W /   62W |    1884MiB /  15356MiB |     46%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA A16                     Off |   00000000:CE:00.0 Off |                    0 |\n",
      "|  0%   39C    P0             32W /   62W |     679MiB /  15356MiB |     53%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA A16                     Off |   00000000:CF:00.0 Off |                    0 |\n",
      "|  0%   41C    P0             33W /   62W |     703MiB /  15356MiB |     31%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA A16                     Off |   00000000:D0:00.0 Off |                    0 |\n",
      "|  0%   36C    P0             32W /   62W |    1206MiB /  15356MiB |     32%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA A16                     Off |   00000000:D1:00.0 Off |                    0 |\n",
      "|  0%   35C    P0             32W /   62W |     679MiB /  15356MiB |     66%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory\n",
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
